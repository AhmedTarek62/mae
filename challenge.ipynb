{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 35px;\">**Waves Lab - Foundation Model Challange!**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the **üì°Radio Foundation Model** Challenge Notebook! \n",
    "\n",
    "This notebook is designed to guide you through the process of adapting our foundational model for various radio tasks. Our model is a Masked Autoencoder (MAE) tailored for radio signal processing built on the vision transformer architecture.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "The primary goal of this challenge is to leverage our model's capabilities to address diverse tasks within the wireless communications domain. Participants are encouraged to fine-tune and adapt the model for applications such as:\n",
    "\n",
    "- ***Signal Classification:*** Identifying different types of radio signals.\n",
    "- ***Channel Estimation:*** Predicting the state of communication channels.\n",
    "- ***Spectrum Sensing:*** Detecting the presence of signals in a frequency band.\n",
    "- ***Signal Reconstruction:*** Reconstructing signals from incomplete or corrupted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚öôÔ∏è **Setup Required**\n",
    "\n",
    "Before running this notebook, please ensure you have configured the necessary parameters in a `.yaml` file similar to `configs/template.yaml` example. This file allows you to define key settings for the fine-tuning experiment, including:\n",
    "\n",
    "- **Base Model**: Specify the path to the pre-trained model and the desired architecture (`model_path`, `model_arch`).\n",
    "- **Task-Specific Parameters**: Set the number of classes, input data size, and any additional model-specific configurations (`num_classes`, `input_size`).\n",
    "- **Dataset Information**: Provide the path to your dataset (`data_path`).\n",
    "- **Training Hyperparameters**: Adjust batch size, learning rate, number of epochs, and more (`batch_size`, `lr`, `epochs`).\n",
    "- **Optimizer and Regularization**: Define weight decay, learning rate scheduling, and drop path rate (`weight_decay`, `blr`, `drop_path`).\n",
    "- **Device and Environment**: Set the device type (`cuda` or `cpu`), random seed, and data loader options (`device`, `seed`, `num_workers`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî Before proceeding with the setup, you'll need to define some key parameters for your fine-tuning experiment. Please make sure to configure the following variables:\n",
    "\n",
    "1. **base_model_arch**:  \n",
    "   Choose the pre-trained model architecture you'd like to fine-tune. The available options are:\n",
    "   - `seg_vit_small_patch16`\n",
    "   - `seg_vit_medium_patch16`\n",
    "   - `seg_vit_large_patch16`\n",
    "\n",
    "2. **base_model_path**:  \n",
    "   Provide the path to the selected pre-trained model architecture on your system.\n",
    "\n",
    "3. **task**:  \n",
    "   Select the task you want to fine-tune the base model for. The available tasks are:\n",
    "   - `Segmentation`\n",
    "   - `Sensing`\n",
    "   - `Custom` (for your own dataset and custom processing functions)\n",
    "\n",
    "4. **data_path**:  \n",
    "   Specify the path to the dataset on your system for the chosen task. This should be the dataset required for your fine-tuning.\n",
    "   \n",
    "Once you have configured these parameters, set the `config_path` variable in the next cell with the path of that `.yaml` file to proceed with running the notebook.\n",
    "\n",
    "**üìù Edit the `.yaml` file now!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = \"/home/elsayedmohammed/mae/configs/positioning.yaml\"    # TODO: Set\n",
    "config_path = \"/home/elsayedmohammed/mae/configs/channel_estimation.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset path provided: /home/elsayedmohammed/datasets/channel_estimation_dataset\n",
      "The outputs path provided: /home/elsayedmohammed/outputs/exp1_channel_estimation\n",
      "==== Loading all the configs..\n",
      "Finetuning on the (channel_estimation) task..\n",
      "==== Setting the device and random seed..\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from FineTuningArgs import FineTuningArgs\n",
    "\n",
    "exp_name = config[\"experiment_name\"]\n",
    "del config[\"experiment_name\"]\n",
    "\n",
    "data_path = config[\"data_path\"]\n",
    "assert os.path.isdir(data_path), print(f\"Incorrect data_path! ({data_path})\")\n",
    "print(f\"The dataset path provided: {data_path}\")\n",
    "\n",
    "output_dir = os.path.join(config[\"output_dir\"], exp_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"The outputs path provided: {output_dir}\")\n",
    "\n",
    "print(f\"==== Loading all the configs..\")\n",
    "config = FineTuningArgs(**config)\n",
    "\n",
    "print(f\"==== Setting the device and random seed..\")\n",
    "device = torch.device(config.device)\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "cudnn.benchmark = True # DEVELOPERS:check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.task == 'segmentation':\n",
    "    from dataset_classes.segmentation_dataset import SegmentationDataset as TaskDataset\n",
    "elif config.task == 'sensing':\n",
    "    from dataset_classes.csi_sensing_dataset import CSISensingDataset as TaskDataset\n",
    "elif config.task == 'signal_identification':\n",
    "    from dataset_classes.radio_signal_identification_dataset import SignalIdentificatio_Dataset as TaskDataset\n",
    "elif config.task == 'positioning':\n",
    "    from dataset_classes.positioning_nr_dataset import PositioningNR as TaskDataset\n",
    "elif config.task == 'channel_estimation':\n",
    "    from dataset_classes.ofdm_channel_estimation_dataset import OfdmChannelEstimation as TaskDataset\n",
    "else:\n",
    "    # TODO\n",
    "    assert False, print(\"Replace this line with import statment \\\n",
    "                         for your dataset class as TasDataset\")\n",
    "    # You can also build your dataset class here in this cell and then change the two following lines accordingly\n",
    "\n",
    "dataset_train = TaskDataset(data_path, split=\"train\")\n",
    "dataset_val = TaskDataset(data_path, split=\"val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into Train and Val objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training dataset\n",
    "## 1. Create the sampling object (Training)\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "## 2. Create the dataloader (Training)\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, sampler=sampler_train,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_mem,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "# For Valdiation dataset\n",
    "## 1. Create the sampling object (Validation)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "## 2. Create the dataloader (Validation)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, sampler=sampler_val,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_mem,\n",
    "        drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elsayedmohammed/AoA-Pruning/pruning-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained checkpoint from: /home/elsayedmohammed/vit-models/pretrained_medium_75.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elsayedmohammed/mae/models/channel_estimation.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing key decoder_pred.weight from pretrained checkpoint\n",
      "Removing key decoder_pred.bias from pretrained checkpoint\n",
      "_IncompatibleKeys(missing_keys=['decoder_pred.weight', 'decoder_pred.bias'], unexpected_keys=['mask_token', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias'])\n",
      "Number of params (M): 3.81\n"
     ]
    }
   ],
   "source": [
    "if config.task == 'segmentation':\n",
    "    import models.segmentation as task_model\n",
    "    assert config.base_arch in list(task_model.__dict__.keys()),\\\n",
    "        print(f\"This model architecture ({config.base_arch}) is not available!\")\n",
    "    model = task_model.__dict__[config.base_arch]() \n",
    "\n",
    "elif config.task == 'sensing':\n",
    "    import models.sensing as task_model\n",
    "    assert config.base_arch in list(task_model.__dict__.keys()),\\\n",
    "        print(f\"This model architecture ({config.base_arch}) is not available!\")\n",
    "    model = task_model.__dict__[config.base_arch](global_pool=config.global_pool,\n",
    "                                                num_classes=config.num_classes,\n",
    "                                                drop_path_rate=config.drop_path)\n",
    "    \n",
    "elif config.task == 'signal_identification':\n",
    "    import models.signal_identification as task_model\n",
    "    assert config.base_arch in list(task_model.__dict__.keys()),\\\n",
    "        print(f\"This model architecture ({config.base_arch}) is not available!\")\n",
    "    model = task_model.__dict__[config.base_arch](global_pool=config.global_pool,\n",
    "                                                num_classes=config.num_classes,\n",
    "                                                drop_path_rate=config.drop_path,\n",
    "                                                in_chans=1)\n",
    "elif config.task == 'positioning':\n",
    "    print(\"POSITIONING\")\n",
    "    scene = \"outdoor\" # TODO: (DEVELOPERS)\n",
    "    tanh = False # TODO: (DEVELOPERS)\n",
    "    import models.positioning as task_model\n",
    "    assert config.base_arch in list(task_model.__dict__.keys()),\\\n",
    "        print(f\"This model architecture ({config.base_arch}) is not available!\")\n",
    "    model = task_model.__dict__[config.base_arch](global_pool=config.global_pool, num_classes=config.num_classes,\n",
    "                                            drop_path_rate=config.drop_path, tanh=tanh,\n",
    "                                            in_chans=4 if scene == 'outdoor' else 5)\n",
    "elif config.task == 'channel_estimation':\n",
    "    import models.channel_estimation as task_model\n",
    "    assert config.base_arch in list(task_model.__dict__.keys()),\\\n",
    "        print(f\"This model architecture ({config.base_arch}) is not available!\")\n",
    "    model = task_model.__dict__[config.base_arch]() \n",
    "\n",
    "else:\n",
    "    # TODO\n",
    "    assert False, print(\"Replace this line with import statment \\\n",
    "                         for your model class as task_model\")\n",
    "    # You can also build your model class here in this cell and then change the two following lines accordingly\n",
    "    #  \n",
    "# Load the model checkpoint\n",
    "print(f\"Loading pre-trained checkpoint from: {config.base_model_path} ...\")\n",
    "msg = model.load_model_checkpoint(checkpoint_path=config.base_model_path)\n",
    "print(msg) # TODO- (DEVELOPERS): why In_IncompatibleKeys?\n",
    "\n",
    "# Freeze the encoder weights (the base)\n",
    "model.freeze_encoder()\n",
    "model.to(device) \n",
    "\n",
    "# Check the model's number of parameters\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of params (M): %.2f' % (n_parameters / 1.e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion selected: MSELoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elsayedmohammed/mae/util/misc.py:76: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# TODO: Feel free to set your own loss function or LR scheduler\n",
    "\n",
    "import util.lr_decay as lrd\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n",
    "\n",
    "param_groups = lrd.param_groups_lrd(model, config.weight_decay, layer_decay=config.layer_decay)\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=config.lr)\n",
    "loss_scaler = NativeScaler()\n",
    "\n",
    "if config.smoothing > 0.:\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=config.smoothing)\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "if config.task == \"channel_estimation\":\n",
    "    from torch.nn import MSELoss\n",
    "    criterion = MSELoss()\n",
    "\n",
    "print(f\"criterion selected: {str(criterion)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.task == \"segmentation\":\n",
    "    from finetuning_engines.segmentation import train_one_epoch, evaluate\n",
    "elif config.task in [\"sensing\", \"signal_identification\"]:\n",
    "    from finetuning_engines.sensing import train_one_epoch, evaluate\n",
    "elif config.task == \"channel_estimation\":\n",
    "    from finetuning_engines.channel_estimation import train_one_epoch, evaluate\n",
    "else:\n",
    "    # TODO\n",
    "    assert False, print(\"Replace this line with import statment \\\n",
    "                         for your finetuing engine script with train_one_epoch and evaluate functions\")\n",
    "    # You can also build your functions here in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10:   0%|          | 0/25 [00:00<?, ?batch/s]/home/elsayedmohammed/AoA-Pruning/pruning-venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1376256])) that is different to the input size (torch.Size([688128, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 0/10:   0%|          | 0/25 [00:16<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (1376256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m least_val_loss, best_stats_epoch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 11\u001b[0m     train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39moutput_dir \u001b[38;5;129;01mand\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m config\u001b[38;5;241m.\u001b[39msave_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     18\u001b[0m         misc\u001b[38;5;241m.\u001b[39msave_model(args\u001b[38;5;241m=\u001b[39mconfig, model\u001b[38;5;241m=\u001b[39mmodel, optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss_scaler\u001b[38;5;241m=\u001b[39mloss_scaler, epoch\u001b[38;5;241m=\u001b[39mepoch)\n",
      "File \u001b[0;32m~/mae/finetuning_engines/channel_estimation.py:89\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, loss_scaler, max_norm, mixup_fn, args)\u001b[0m\n\u001b[1;32m     87\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets, snr)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m math\u001b[38;5;241m.\u001b[39misfinite(loss_value):\n",
      "File \u001b[0;32m~/AoA-Pruning/pruning-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AoA-Pruning/pruning-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/AoA-Pruning/pruning-venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:538\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AoA-Pruning/pruning-venv/lib/python3.10/site-packages/torch/nn/functional.py:3383\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3381\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3383\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/AoA-Pruning/pruning-venv/lib/python3.10/site-packages/torch/functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (1376256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import util.misc as misc\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "print(f\"Training for {config.epochs} epochs..\")\n",
    "start_time = time.time()\n",
    "least_val_loss, best_stats_epoch = np.inf, 0\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train,\n",
    "        optimizer, device, epoch, loss_scaler,\n",
    "        config.clip_grad, None,\n",
    "        args=config\n",
    "    )\n",
    "    if config.output_dir and (epoch % config.save_every == 0):\n",
    "        misc.save_model(args=config, model=model, optimizer=optimizer, loss_scaler=loss_scaler, epoch=epoch)\n",
    "\n",
    "    val_stats = evaluate(data_loader_val, model, criterion, device)\n",
    "    if val_stats[\"avg_loss\"] < least_val_loss:\n",
    "         least_val_loss = val_stats[\"avg_loss\"]\n",
    "         best_stats_epoch = epoch\n",
    "\n",
    "    if config.output_dir:\n",
    "        log_stats = {\"epoch\": epoch,\n",
    "                     \"train_loss\": train_stats[\"avg_loss\"],\n",
    "                     \"val_loss\": val_stats[\"avg_loss\"],\n",
    "                     \"val_acc\": val_stats[\"avg_acc\"]}\n",
    "        with open(os.path.join(config.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to report the finetuning results in the following `.txt` format to enable evaluation.\n",
    "\n",
    "Our evaluation takes into account:\n",
    " - Number of data samples\n",
    " - Number of model parameters\n",
    " - The reported loss/accuracy\n",
    "\n",
    "**Required Format:**\n",
    "```\n",
    "{\n",
    "    task:               'task_name(arbitrary)',\n",
    "    loss:               validation final loss (if regression) or None otherwise,\n",
    "    accuracy:           validation final accurcay (if classification) or None otherwise,\n",
    "    model_n_parameters: number of model parameters,\n",
    "    validation_length:  number of data samples in your validation dataset\n",
    "    score:              the score assigned to your task (automatically generated)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.misc import report_score\n",
    "\n",
    "report = report_score(config, model, dataset_val, least_val_loss, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (DEVELOPERS): Just for us, going to delete this cell soon!\n",
    "\n",
    "validation_length = len(dataset_val)\n",
    "print(f\"validation_length = {validation_length}\")\n",
    "\n",
    "validation_score = min(validation_length // 100 * 10, 100)\n",
    "print(f\"validation_score = {validation_score}\")\n",
    "\n",
    "print(least_val_loss)\n",
    "print(f\"performance (loss) = {least_val_loss}\")\n",
    "\n",
    "perf_score = 100 - least_val_loss*100\n",
    "print(f\"paras_score = {perf_score}\")\n",
    "\n",
    "model_n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"parameteres = {model_n_parameters}\")\n",
    "\n",
    "paras_score = 100 - min(max(model_n_parameters // 200000 * 5, 0), 100) \n",
    "print(f\"paras_score = {paras_score}\")\n",
    "\n",
    "print(f\"===total = {0.5*perf_score + 0.25*paras_score + 0.25*validation_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
